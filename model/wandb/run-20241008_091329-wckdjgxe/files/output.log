/home/zhicao/ODE/model/model_classes.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.beta = nn.Parameter(torch.tensor(learnable_params['beta'], device=device))
/home/zhicao/ODE/model/model_classes.py:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.alpha = nn.Parameter(torch.tensor(learnable_params['alpha'], device=device))
/home/zhicao/ODE/model/model_classes.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.gamma = nn.Parameter(torch.tensor(learnable_params['gamma'], device=device))
/home/zhicao/ODE/model/model_classes.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).
  self.mu = nn.Parameter(torch.tensor(learnable_params['mu'], device=device))
Epoch 0, Batch 0, Loss X: 0.6236798763275146, Loss Y: 0.5739275217056274
Epoch 0, Batch 1, Loss X: 0.8393392562866211, Loss Y: 0.7575118541717529
Epoch 0, Batch 2, Loss X: 0.6229140758514404, Loss Y: 0.5726404786109924
Epoch 0, Batch 3, Loss X: 0.8384133577346802, Loss Y: 0.7558128237724304
Epoch 0, Batch 4, Loss X: 0.8558068871498108, Loss Y: 0.7606284022331238
Epoch 0, Batch 5, Loss X: 1.0600323677062988, Loss Y: 0.9375600814819336
Epoch 0, Batch 6, Loss X: 0.8548631072044373, Loss Y: 0.7587973475456238
Epoch 0, Batch 7, Loss X: 1.0589747428894043, Loss Y: 0.9353989362716675
Epoch 0, Batch 8, Loss X: 0.9400808811187744, Loss Y: 0.8272117972373962
Epoch 0, Batch 9, Loss X: 1.1392827033996582, Loss Y: 1.000548243522644
Epoch 0, Batch 10, Loss X: 0.9391186833381653, Loss Y: 0.825136125087738
Epoch 0, Batch 11, Loss X: 1.138225793838501, Loss Y: 0.9981589913368225
Epoch 0, Batch 12, Loss X: 1.001415491104126, Loss Y: 0.8747192621231079
Epoch 0, Batch 13, Loss X: 1.1981359720230103, Loss Y: 1.0463056564331055
Epoch 0, Batch 14, Loss X: 1.0004488229751587, Loss Y: 0.872450053691864
Epoch 0, Batch 15, Loss X: 1.1970770359039307, Loss Y: 1.0437449216842651
Epoch 0, Batch 16, Loss X: 1.0464177131652832, Loss Y: 0.9086322784423828
Epoch 0, Batch 17, Loss X: 1.2420858144760132, Loss Y: 1.0795156955718994
Epoch 0, Batch 18, Loss X: 1.0454230308532715, Loss Y: 0.9062386751174927
Epoch 0, Batch 19, Loss X: 1.2410001754760742, Loss Y: 1.0768481492996216
Epoch 0, Batch 20, Loss X: 0.17708200216293335, Loss Y: 0.21893924474716187
Epoch 0, Batch 21, Loss X: 0.33220037817955017, Loss Y: 0.3303770422935486
Epoch 0, Batch 22, Loss X: 0.17695510387420654, Loss Y: 0.219013050198555
Epoch 0, Batch 23, Loss X: 0.3318732976913452, Loss Y: 0.3298737108707428
Epoch 0, Batch 24, Loss X: 0.33239904046058655, Loss Y: 0.31765592098236084
Epoch 0, Batch 25, Loss X: 0.5414588451385498, Loss Y: 0.48557722568511963
Epoch 0, Batch 26, Loss X: 0.3320769965648651, Loss Y: 0.31710904836654663
Epoch 0, Batch 27, Loss X: 0.5410060286521912, Loss Y: 0.4846348166465759
Epoch 0, Batch 28, Loss X: 0.40678703784942627, Loss Y: 0.36964449286460876
Epoch 0, Batch 29, Loss X: 0.6244714856147766, Loss Y: 0.5480377674102783
Epoch 0, Batch 30, Loss X: 0.40640926361083984, Loss Y: 0.3689173460006714
Epoch 0, Batch 31, Loss X: 0.6239738464355469, Loss Y: 0.5469464659690857
Epoch 0, Batch 32, Loss X: 0.4632819890975952, Loss Y: 0.409564733505249
Epoch 0, Batch 33, Loss X: 0.6862325072288513, Loss Y: 0.5943517684936523
Epoch 0, Batch 34, Loss X: 0.4628432095050812, Loss Y: 0.4086794853210449
Epoch 0, Batch 35, Loss X: 0.6856696605682373, Loss Y: 0.593102753162384
Epoch 0, Batch 36, Loss X: 0.5059064030647278, Loss Y: 0.439617395401001
Epoch 0, Batch 37, Loss X: 0.7327063083648682, Loss Y: 0.6288532614707947
Epoch 0, Batch 38, Loss X: 0.5054038166999817, Loss Y: 0.43858277797698975
Epoch 0, Batch 39, Loss X: 0.7320706844329834, Loss Y: 0.6274450421333313
Epoch 0, Batch 40, Loss X: 0.17292216420173645, Loss Y: 0.28724852204322815
Epoch 0, Batch 41, Loss X: 0.1634548306465149, Loss Y: 0.2746177911758423
Epoch 0, Batch 42, Loss X: 0.17304326593875885, Loss Y: 0.2879287898540497
Epoch 0, Batch 43, Loss X: 0.16351644694805145, Loss Y: 0.275113970041275
Epoch 0, Batch 44, Loss X: 0.15068650245666504, Loss Y: 0.25761693716049194
Epoch 0, Batch 45, Loss X: 0.12389004230499268, Loss Y: 0.21624231338500977
Epoch 0, Batch 46, Loss X: 0.15063335001468658, Loss Y: 0.2577414810657501
Epoch 0, Batch 47, Loss X: 0.12381263822317123, Loss Y: 0.21626374125480652
Epoch 0, Batch 48, Loss X: 0.13198164105415344, Loss Y: 0.23100058734416962
Epoch 0, Batch 49, Loss X: 0.10716482996940613, Loss Y: 0.18463289737701416
Epoch 0, Batch 50, Loss X: 0.13184931874275208, Loss Y: 0.230849951505661
Epoch 0, Batch 51, Loss X: 0.10705186426639557, Loss Y: 0.18448585271835327
Epoch 0, Batch 52, Loss X: 0.11516722291707993, Loss Y: 0.2054632306098938
Epoch 0, Batch 53, Loss X: 0.0998770073056221, Loss Y: 0.16348671913146973
Epoch 0, Batch 54, Loss X: 0.11501235514879227, Loss Y: 0.20520047843456268
Epoch 0, Batch 55, Loss X: 0.09977385401725769, Loss Y: 0.16331137716770172
Epoch 0, Batch 56, Loss X: 0.10217700898647308, Loss Y: 0.1842077523469925
Epoch 0, Batch 57, Loss X: 0.09888899326324463, Loss Y: 0.15073274075984955
Epoch 0, Batch 58, Loss X: 0.1020253598690033, Loss Y: 0.18392278254032135
Epoch 0, Batch 59, Loss X: 0.09881143271923065, Loss Y: 0.15058614313602448
Epoch 0, Batch 60, Loss X: 0.17833232879638672, Loss Y: 0.2932905852794647
Epoch 0, Batch 61, Loss X: 0.17822130024433136, Loss Y: 0.2929672300815582
Epoch 0, Batch 62, Loss X: 0.1780625283718109, Loss Y: 0.29274752736091614
Epoch 0, Batch 63, Loss X: 0.17793293297290802, Loss Y: 0.29236945509910583
Epoch 0, Batch 64, Loss X: 0.17619109153747559, Loss Y: 0.28972265124320984
Epoch 0, Batch 65, Loss X: 0.17575158178806305, Loss Y: 0.28891995549201965
Epoch 0, Batch 66, Loss X: 0.17585526406764984, Loss Y: 0.2889963984489441
Epoch 0, Batch 67, Loss X: 0.17540179193019867, Loss Y: 0.28815987706184387
Epoch 0, Batch 68, Loss X: 0.17322777211666107, Loss Y: 0.2849230468273163
Epoch 0, Batch 69, Loss X: 0.17245975136756897, Loss Y: 0.2836652100086212
Epoch 0, Batch 70, Loss X: 0.17284716665744781, Loss Y: 0.28408515453338623
Epoch 0, Batch 71, Loss X: 0.17207127809524536, Loss Y: 0.2828076183795929
Epoch 0, Batch 72, Loss X: 0.1689855456352234, Loss Y: 0.2783810496330261
Epoch 0, Batch 73, Loss X: 0.16783998906612396, Loss Y: 0.27661871910095215
Epoch 0, Batch 74, Loss X: 0.16857804358005524, Loss Y: 0.27747979760169983
Epoch 0, Batch 75, Loss X: 0.16742807626724243, Loss Y: 0.27570751309394836
Epoch 0, Batch 76, Loss X: 0.16424745321273804, Loss Y: 0.27115046977996826
Epoch 0, Batch 77, Loss X: 0.16270114481449127, Loss Y: 0.26885753870010376
Epoch 0, Batch 78, Loss X: 0.16382573544979095, Loss Y: 0.27021661400794983
Epoch 0, Batch 79, Loss X: 0.1622779667377472, Loss Y: 0.26792076230049133
Epoch 0, Batch 80, Loss X: 0.1763685643672943, Loss Y: 0.28553304076194763
Epoch 0, Batch 81, Loss X: 0.1762026995420456, Loss Y: 0.2850179672241211
Epoch 0, Batch 82, Loss X: 0.17591485381126404, Loss Y: 0.28453633189201355
Epoch 0, Batch 83, Loss X: 0.17574308812618256, Loss Y: 0.2840137481689453
Epoch 0, Batch 84, Loss X: 0.17442825436592102, Loss Y: 0.281843900680542
Epoch 0, Batch 85, Loss X: 0.17418941855430603, Loss Y: 0.2812312841415405
Epoch 0, Batch 86, Loss X: 0.1739628165960312, Loss Y: 0.2808261513710022
Epoch 0, Batch 87, Loss X: 0.1737210750579834, Loss Y: 0.28020980954170227
Epoch 0, Batch 88, Loss X: 0.17163822054862976, Loss Y: 0.27706384658813477
Epoch 0, Batch 89, Loss X: 0.17132681608200073, Loss Y: 0.27636224031448364
Epoch 0, Batch 90, Loss X: 0.1711665838956833, Loss Y: 0.2760401964187622
Epoch 0, Batch 91, Loss X: 0.17085525393486023, Loss Y: 0.275337815284729
Epoch 0, Batch 92, Loss X: 0.16795367002487183, Loss Y: 0.27115514874458313
Epoch 0, Batch 93, Loss X: 0.16755977272987366, Loss Y: 0.27035051584243774
Epoch 0, Batch 94, Loss X: 0.167480930685997, Loss Y: 0.27013811469078064
Epoch 0, Batch 95, Loss X: 0.16708728671073914, Loss Y: 0.26933524012565613
Epoch 0, Batch 96, Loss X: 0.1639072299003601, Loss Y: 0.2648063600063324
Epoch 0, Batch 97, Loss X: 0.16341976821422577, Loss Y: 0.26389050483703613
Epoch 0, Batch 98, Loss X: 0.16343557834625244, Loss Y: 0.26380565762519836
Epoch 0, Batch 99, Loss X: 0.1629483699798584, Loss Y: 0.26289400458335876
Epoch 0 Average Loss X: 0.38856832817196846, Average Loss Y: 0.4165274375677109
Epoch 1, Batch 0, Loss X: 0.6181471347808838, Loss Y: 0.5694683790206909
Epoch 1, Batch 1, Loss X: 0.8331349492073059, Loss Y: 0.7534111738204956
Epoch 1, Batch 2, Loss X: 0.6185806393623352, Loss Y: 0.5702193379402161
Epoch 1, Batch 3, Loss X: 0.8334935903549194, Loss Y: 0.7540652751922607
Epoch 1, Batch 4, Loss X: 0.8511492609977722, Loss Y: 0.7600783705711365
Epoch 1, Batch 5, Loss X: 1.0551397800445557, Loss Y: 0.9382593631744385
Epoch 1, Batch 6, Loss X: 0.8509451150894165, Loss Y: 0.7598121166229248
Epoch 1, Batch 7, Loss X: 1.0547289848327637, Loss Y: 0.9376306533813477
Epoch 1, Batch 8, Loss X: 0.9363210201263428, Loss Y: 0.8298186659812927
Epoch 1, Batch 9, Loss X: 1.1351650953292847, Loss Y: 1.004343032836914
Epoch 1, Batch 10, Loss X: 0.9355124235153198, Loss Y: 0.8285057544708252
Epoch 1, Batch 11, Loss X: 1.1341454982757568, Loss Y: 1.0026366710662842
Epoch 1, Batch 12, Loss X: 0.9976028800010681, Loss Y: 0.8790423274040222
Epoch 1, Batch 13, Loss X: 1.193779468536377, Loss Y: 1.0516455173492432
Epoch 1, Batch 14, Loss X: 0.9963878989219666, Loss Y: 0.8769954442977905
Epoch 1, Batch 15, Loss X: 1.1923662424087524, Loss Y: 1.0492186546325684
Epoch 1, Batch 16, Loss X: 1.0418914556503296, Loss Y: 0.9136343002319336
Epoch 1, Batch 17, Loss X: 1.236842393875122, Loss Y: 1.085351586341858
Epoch 1, Batch 18, Loss X: 1.0404105186462402, Loss Y: 0.911117672920227
Epoch 1, Batch 19, Loss X: 1.2351789474487305, Loss Y: 1.0824726819992065
Epoch 1, Batch 20, Loss X: 0.17675931751728058, Loss Y: 0.21781601011753082
Epoch 1, Batch 21, Loss X: 0.33036917448043823, Loss Y: 0.33047252893447876
Epoch 1, Batch 22, Loss X: 0.17660926282405853, Loss Y: 0.2178867608308792
Epoch 1, Batch 23, Loss X: 0.3298908472061157, Loss Y: 0.32991886138916016
Epoch 1, Batch 24, Loss X: 0.329966276884079, Loss Y: 0.318095862865448
Epoch 1, Batch 25, Loss X: 0.5374780297279358, Loss Y: 0.4872550964355469
Epoch 1, Batch 26, Loss X: 0.3294798731803894, Loss Y: 0.31752896308898926
Epoch 1, Batch 27, Loss X: 0.5367734432220459, Loss Y: 0.48628515005111694
Epoch 1, Batch 28, Loss X: 0.403246134519577, Loss Y: 0.37074586749076843
Epoch 1, Batch 29, Loss X: 0.6193658709526062, Loss Y: 0.5503749847412109
Epoch 1, Batch 30, Loss X: 0.40267443656921387, Loss Y: 0.3700198531150818
Epoch 1, Batch 31, Loss X: 0.6186114549636841, Loss Y: 0.5492735505104065
Epoch 1, Batch 32, Loss X: 0.4588107466697693, Loss Y: 0.4112037420272827
Epoch 1, Batch 33, Loss X: 0.6801987290382385, Loss Y: 0.5971981883049011
Epoch 1, Batch 34, Loss X: 0.4581925570964813, Loss Y: 0.4103051424026489
Epoch 1, Batch 35, Loss X: 0.679418683052063, Loss Y: 0.5958964824676514
Epoch 1, Batch 36, Loss X: 0.5007091760635376, Loss Y: 0.4416075050830841
Epoch 1, Batch 37, Loss X: 0.725968599319458, Loss Y: 0.6319614052772522
Epoch 1, Batch 38, Loss X: 0.5000740885734558, Loss Y: 0.44047367572784424
Epoch 1, Batch 39, Loss X: 0.7251790165901184, Loss Y: 0.6303774118423462
Epoch 1, Batch 40, Loss X: 0.17593702673912048, Loss Y: 0.2844425439834595
Epoch 1, Batch 41, Loss X: 0.16631852090358734, Loss Y: 0.2719155251979828
Epoch 1, Batch 42, Loss X: 0.1761265993118286, Loss Y: 0.28525352478027344
Epoch 1, Batch 43, Loss X: 0.1664394587278366, Loss Y: 0.2725182771682739
Epoch 1, Batch 44, Loss X: 0.1533878743648529, Loss Y: 0.255063533782959
Epoch 1, Batch 45, Loss X: 0.1258426159620285, Loss Y: 0.21398355066776276
Epoch 1, Batch 46, Loss X: 0.15337875485420227, Loss Y: 0.2552487552165985
Epoch 1, Batch 47, Loss X: 0.12579572200775146, Loss Y: 0.21405352652072906
Epoch 1, Batch 48, Loss X: 0.13433760404586792, Loss Y: 0.22859396040439606
Epoch 1, Batch 49, Loss X: 0.10839816927909851, Loss Y: 0.18268318474292755
Epoch 1, Batch 50, Loss X: 0.13423758745193481, Loss Y: 0.22847308218479156
Epoch 1, Batch 51, Loss X: 0.10830138623714447, Loss Y: 0.18257245421409607
Epoch 1, Batch 52, Loss X: 0.1171380802989006, Loss Y: 0.203191876411438
Epoch 1, Batch 53, Loss X: 0.1004398986697197, Loss Y: 0.16186130046844482
Epoch 1, Batch 54, Loss X: 0.1170089915394783, Loss Y: 0.2029508650302887
Epoch 1, Batch 55, Loss X: 0.10034314543008804, Loss Y: 0.16173090040683746
Epoch 1, Batch 56, Loss X: 0.10378237068653107, Loss Y: 0.1820802092552185
Epoch 1, Batch 57, Loss X: 0.09887894988059998, Loss Y: 0.1494482457637787
Epoch 1, Batch 58, Loss X: 0.10365154594182968, Loss Y: 0.18182337284088135
Epoch 1, Batch 59, Loss X: 0.09880131483078003, Loss Y: 0.14935916662216187
Epoch 1, Batch 60, Loss X: 0.1816895306110382, Loss Y: 0.2901114225387573
Epoch 1, Batch 61, Loss X: 0.18159109354019165, Loss Y: 0.2897457480430603
Epoch 1, Batch 62, Loss X: 0.18147322535514832, Loss Y: 0.28953641653060913
Epoch 1, Batch 63, Loss X: 0.18136411905288696, Loss Y: 0.28908246755599976
Epoch 1, Batch 64, Loss X: 0.17960374057292938, Loss Y: 0.28628671169281006
Epoch 1, Batch 65, Loss X: 0.17918799817562103, Loss Y: 0.2853713929653168
Epoch 1, Batch 66, Loss X: 0.17935214936733246, Loss Y: 0.2853739857673645
Epoch 1, Batch 67, Loss X: 0.17893025279045105, Loss Y: 0.2843824625015259
Epoch 1, Batch 68, Loss X: 0.1767483949661255, Loss Y: 0.280926376581192
Epoch 1, Batch 69, Loss X: 0.17601269483566284, Loss Y: 0.2794996500015259
Epoch 1, Batch 70, Loss X: 0.17647619545459747, Loss Y: 0.27980437874794006
Epoch 1, Batch 71, Loss X: 0.1757340133190155, Loss Y: 0.2783726453781128
Epoch 1, Batch 72, Loss X: 0.17263351380825043, Loss Y: 0.2737630307674408
Epoch 1, Batch 73, Loss X: 0.17151446640491486, Loss Y: 0.271879643201828
Epoch 1, Batch 74, Loss X: 0.17233406007289886, Loss Y: 0.2726638615131378
Epoch 1, Batch 75, Loss X: 0.17120961844921112, Loss Y: 0.27079248428344727
Epoch 1, Batch 76, Loss X: 0.16801214218139648, Loss Y: 0.26609957218170166
Epoch 1, Batch 77, Loss X: 0.16648568212985992, Loss Y: 0.263732373714447
Epoch 1, Batch 78, Loss X: 0.16770438849925995, Loss Y: 0.26503604650497437
Epoch 1, Batch 79, Loss X: 0.16618262231349945, Loss Y: 0.2626822888851166
Epoch 1, Batch 80, Loss X: 0.18051035702228546, Loss Y: 0.27993571758270264
Epoch 1, Batch 81, Loss X: 0.18040622770786285, Loss Y: 0.2793520987033844
Epoch 1, Batch 82, Loss X: 0.18020659685134888, Loss Y: 0.2788793742656708
Epoch 1, Batch 83, Loss X: 0.1801028549671173, Loss Y: 0.2783096134662628
Epoch 1, Batch 84, Loss X: 0.17881743609905243, Loss Y: 0.27605509757995605
Epoch 1, Batch 85, Loss X: 0.1786496341228485, Loss Y: 0.2754169702529907
Epoch 1, Batch 86, Loss X: 0.1785207837820053, Loss Y: 0.2750592827796936
Epoch 1, Batch 87, Loss X: 0.17835289239883423, Loss Y: 0.27443528175354004
Epoch 1, Batch 88, Loss X: 0.1762932389974594, Loss Y: 0.2712520360946655
Epoch 1, Batch 89, Loss X: 0.17605596780776978, Loss Y: 0.27055981755256653
Epoch 1, Batch 90, Loss X: 0.17599831521511078, Loss Y: 0.2703147828578949
Epoch 1, Batch 91, Loss X: 0.1757625937461853, Loss Y: 0.2696351706981659
Epoch 1, Batch 92, Loss X: 0.1728716343641281, Loss Y: 0.2654549777507782
Epoch 1, Batch 93, Loss X: 0.17255213856697083, Loss Y: 0.2646861672401428
Epoch 1, Batch 94, Loss X: 0.1725793033838272, Loss Y: 0.2645714581012726
Epoch 1, Batch 95, Loss X: 0.17226113379001617, Loss Y: 0.2638140022754669
Epoch 1, Batch 96, Loss X: 0.16908587515354156, Loss Y: 0.2593105435371399
Epoch 1, Batch 97, Loss X: 0.16867198050022125, Loss Y: 0.25844958424568176
Epoch 1, Batch 98, Loss X: 0.16879743337631226, Loss Y: 0.25847572088241577
Epoch 1, Batch 99, Loss X: 0.16838482022285461, Loss Y: 0.2576250433921814
Epoch 1 Average Loss X: 0.38888365752995013, Average Loss Y: 0.41486407577991485
Epoch 2, Batch 0, Loss X: 0.6069825291633606, Loss Y: 0.5805678367614746
Epoch 2, Batch 1, Loss X: 0.8190292716026306, Loss Y: 0.7664011716842651
Epoch 2, Batch 2, Loss X: 0.6071918606758118, Loss Y: 0.5813038945198059
Epoch 2, Batch 3, Loss X: 0.8192172646522522, Loss Y: 0.7669867277145386
Traceback (most recent call last):
  File "train.py", line 254, in <module>
    run(
  File "train.py", line 232, in run
    train_and_evaluate(model, x_batches, y_batches, a_batches, batch_size, optim_config=optim_config, device=device, expert=expert)
  File "train.py", line 178, in train_and_evaluate
    total_loss.backward()
  File "/home/zhicao/miniconda3/envs/hybrid/lib/python3.8/site-packages/torch/_tensor.py", line 521, in backward
    torch.autograd.backward(
  File "/home/zhicao/miniconda3/envs/hybrid/lib/python3.8/site-packages/torch/autograd/__init__.py", line 289, in backward
    _engine_run_backward(
  File "/home/zhicao/miniconda3/envs/hybrid/lib/python3.8/site-packages/torch/autograd/graph.py", line 768, in _engine_run_backward
    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass
  File "/home/zhicao/miniconda3/envs/hybrid/lib/python3.8/site-packages/torch/autograd/function.py", line 291, in apply
    def apply(self, *args):
KeyboardInterrupt
